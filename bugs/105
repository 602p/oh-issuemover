{"status": "resolved", "priority": "bug", "title": "Bugzilla importer fetches XML data for trackers with fresh bugs", "milestone": "", "superceder": "", "nosylist": ["paulproteus, pythonian4000"], "assigned": "", "waitingon": "", "keywords": [], "id": "105", "files": [{"url": "http://openhatch.org/bugs/file50/add_Timestamp_and_change_logging_to_error.patch", "author": "pythonian4000"}, {"url": "http://openhatch.org/bugs/file55/fix_Bugzilla_URL_freshness_tests.patch", "author": "pythonian4000"}, {"url": "http://openhatch.org/bugs/file53/test_Timestamps_of_Bugzilla_tracker_URLs.patch", "author": "pythonian4000"}], "history": [{"message": "Resolved to my satisfaction! Test passes. Deployed on linode2. This code will be\nactive during the next crawl. (-:\n   \n", "author": "paulproteus"}, {"message": "Here is a fix for the test_bugzilla_importing_hits_network_if_urls_are_not_fresh\nfailure.\n   \n", "author": "pythonian4000"}, {"message": "I wrote some extra tests. Here are your next steps:\n\n$ git fetch origin\n$ git rebase origin/bugzilla_freshness\n\nI merged in these commits, but I added some on top. Take a look at the new\nmysite.search.tests.BugzillaImporterOnlyPerformsAQueryOncePerDay class.\n\nFor the \"Some URLs are too long for the way we're doing this\" problem, I wrote a\ntest method in mysite.search.tests.BugzillaImporterOnlyPerformsAQueryOncePerDay\nthat shows it's fixed. (I changed the function to hash the URLs first.)\n\nBut there's a test in there that doesn't pass. I think the failure of the\ntest_bugzilla_importing_hits_network_if_urls_are_not_fresh method shows an\nactual problem... unless it's a mistake on my part. What do you think?\n   \n", "author": "paulproteus"}, {"message": "Sure =) here is the test.\n\nAs I thought, it is a MySQL problem. Using './bin/mysite' to run the test causes\nfailure whereas './bin/sqlite_mysite' has no problems. My guess is it is related\nto the length of the string supplied to the Timestamp update function - it fails\non the excessively long Miro URL. While the URL itself can be easily shortened,\nnonetheless it exposes a weakness in the underlying MySQL.\n   \n", "author": "pythonian4000"}, {"message": "I think this deserves a test. Would you be willing to add a patch to this series\nthat adds a test?\n   \n", "author": "paulproteus"}, {"message": "For now, here is a patch that checks the Timestamp of all main XML query URLs in\nthe Bugzilla importer. There is an error catcher in there because I was getting\nsome odd MySQL errors when it came to updating the timestamp of a URL - first\nsome odd error I ddin't recognise, then on the second run an Integrity Error due\nto duplicate keys (probably due to the previos error). Not sure exactly how\nTimestamp works, so unsure as to the underlying cause, but the catch stops\nthings breaking.\n\n(Also changed a couple of calls to 'logging.info' to 'logging.error' for\ncorrectness.)\n   \n", "author": "pythonian4000"}, {"message": "We get data from Bugzilla trackers in one go, rather than per bug (it is kinder\non the tracking servers - many less individual requests). The problem with this\nis that this grab is also used to get the list of bug ids to check, so by the\ntime we get to checking if a particular bug is fresh, we have already got the\ndata for it and our check is pointless. For the Bugzilla importer then, we also\nneed to check if the XML buglist has been downloaded within the last day.\n\nTalked with paulproteus on IRC, and it was suggested that the Timestamp class be\nutilised on the bug tracker query URL. This could either just be implemented in\nthe single BugzillaBug class, or we could go further and have a Timestamp check\nfor every GET we make via a helper (this would not necessarily be useful for\ne.g. profile imports).\n   \n", "author": "pythonian4000"}]}