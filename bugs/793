{"status": "unread", "priority": "wish", "title": "oh-bugimporters should do per-domain backoff", "milestone": "", "superceder": "", "nosylist": ["paulproteus"], "assigned": "", "waitingon": "", "keywords": [], "id": "793", "files": [], "history": [{"message": "Some bug trackers (openhatch.org/bugs/ especially...) if you request more than 1-\n2 bugs per second report HTTP 504 Gateway Timeout.\n\nThe way Scrapy handles this now is in the \n<a href=\"http://doc.scrapy.org/en/0.12/topics/downloader-middleware.html#module-\">http://doc.scrapy.org/en/0.12/topics/downloader-middleware.html#module-</a>\nscrapy.contrib.downloadermiddleware.retry middleware, which re-queues the job but \ndoesn't insist on a time delay.\n\nIt'd be nice to have a custom RetryMiddleware that did per-domain backoff. (Note \nthat we're sort of abusing the Scrapy architecture; we're supposed to have one \n\"spider\" class per domain, but instead we only have one.)\n\nOne way to do this is to provide a custom subclass of \nscrapy.contrib.downloadermiddleware.retry.RetryMiddleware and then override the \n_retry method.\n\nThat should let us more reliably crawl some of the sites that are quite finnicky.\n   \n", "author": "paulproteus"}]}